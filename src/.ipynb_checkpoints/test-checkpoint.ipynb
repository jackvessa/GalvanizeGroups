{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "# Imports for NLP Analysis of Columns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create Group Sizes\n",
    "def calc_group_sizes(num_students, num_groups):\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    num_students : int\n",
    "        Number of students in the class\n",
    "    num_groups : int\n",
    "        Number of groups to break students into\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    group_size : List of ideal group sizes\n",
    "    '''\n",
    "    group_sizes = []\n",
    "\n",
    "    class_size = int(num_students)\n",
    "    group_num_count = int(num_groups)\n",
    "    group_num = int(num_groups)\n",
    "\n",
    "    for i in range(group_num_count):\n",
    "        temp = class_size // group_num\n",
    "        class_size -= temp\n",
    "        group_num -= 1\n",
    "        group_sizes.append(temp)\n",
    "\n",
    "    return group_sizes\n",
    "\n",
    "# Clean DataFrame by Section\n",
    "def clean_file(dataframe):\n",
    "    '''\n",
    "    Clean CSV file\n",
    "    --------------------\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    .csv file :\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    Pandas DataFrame (Cleaned)\n",
    "    '''\n",
    "    df = dataframe.copy()\n",
    "    df.set_index(keys=df['FIRST'],inplace=True)\n",
    "\n",
    "    try:\n",
    "        df.drop(columns=['FIRST','LAST','GITHUB','Class Rank','Overall Average'],inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].str.rstrip('%').astype('float') / 100.0\n",
    "    \n",
    "#     df = df.select_dtypes(exclude=['object','bool'])\n",
    "\n",
    "#     df.drop(columns=['id','section_sis_id','attempt'],inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# def clean_file_galvanize(df):\n",
    "#     '''\n",
    "#     '''\n",
    "#     temp_df = df.copy()\n",
    "#     temp_df.drop(columns=['LAST', 'GITHUB','Class Rank'], inplace=True)\n",
    "\n",
    "#     temp_df.set_index('FIRST', inplace=True)\n",
    "\n",
    "#     for col in temp_df.columns:\n",
    "\n",
    "#         temp_df[col] = temp_df[col].apply(lambda x: float((str(x).replace(\"%\",\"\"))))\n",
    "\n",
    "#     return temp_df\n",
    "\n",
    "\n",
    "# Normalize DataFrame (0-1)\n",
    "def normalize_df(df):\n",
    "    '''\n",
    "    Normalize DataFrame Values from 0-1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame to Normalize\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Normalized DataFrame\n",
    "    '''\n",
    "    return df.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)) \\\n",
    "        if np.min(x) != np.max(x) else x)\n",
    "\n",
    "# Generate Optimized Group (Based on Residual Sum of Squares)\n",
    "def generate_optimized_groups(student_df, num_iter = 100, num_groups = 6, Homogeneous = 0, criteria = 'score'):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    student_df : DataFrame with student names as index and score column\n",
    "    num_iter : int\n",
    "        Number of Iterations to run loss function\n",
    "    num_groups : int\n",
    "        Number of groups to divide students into\n",
    "    Homogeneous : bool\n",
    "        If True, create Homogeneous (similar) groups.\n",
    "        If False, create Heterogeneous (different) groups\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Optimal Groups\n",
    "    '''\n",
    "    index_list = list(student_df.index)\n",
    "\n",
    "    if Homogeneous == 0:\n",
    "        ideal_loss = 9999\n",
    "    elif Homogeneous == 1:\n",
    "        ideal_loss = 0\n",
    "    num_students = len(student_df)\n",
    "\n",
    "    size_list = calc_group_sizes(num_students,num_groups)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        randomized_index_list = np.random.choice(index_list, size = len(index_list),replace=False)\n",
    "        group_set = set({})\n",
    "        index_track = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for num in size_list:\n",
    "            j = frozenset(randomized_index_list[0 + index_track:index_track+num])\n",
    "            group_set.add(j)\n",
    "            index_track += num\n",
    "\n",
    "        for group in group_set:\n",
    "            unfrozen = set(group)\n",
    "            group_loss = 0\n",
    "            avg_score = np.mean(student_df.loc[unfrozen][criteria])\n",
    "\n",
    "            for s in range(len(group)):\n",
    "                group_loss += (student_df.loc[unfrozen][criteria][s] - avg_score) ** 2\n",
    "\n",
    "            total_loss += group_loss\n",
    "\n",
    "        if Homogeneous == 0 and total_loss < ideal_loss:\n",
    "            ideal_loss = total_loss\n",
    "            best_group = group_set\n",
    "            print(\"New Best Homogeneous Group Loss:\", ideal_loss)\n",
    "\n",
    "        elif Homogeneous == 1 and total_loss > ideal_loss:\n",
    "            ideal_loss = total_loss\n",
    "            best_group = group_set\n",
    "            print(\"New Best Heterogeneous Group Loss:\", ideal_loss)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Final Best Group Loss:\", ideal_loss)\n",
    "    print(\"Final Best Grouping:\\n\")\n",
    "\n",
    "\n",
    "    for i,g in enumerate(best_group):\n",
    "        print(\"Group\",i+1)\n",
    "        print(student_df.loc[set(g)][criteria],\"\\n\")\n",
    "\n",
    "    return best_group\n",
    "\n",
    "# Clean File Based on All Gradebook Assignments and Section\n",
    "def clean_file_all_assignments(dataframe,sectionID):\n",
    "    '''\n",
    "    Clean CSV file\n",
    "    --------------------\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    .csv file :\n",
    "    sectionID : Class/Period Number to Group\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    Pandas DataFrame (Cleaned)\n",
    "    '''\n",
    "    sectionID = str(sectionID)\n",
    "    df = dataframe.copy()\n",
    "    df.drop(df.index[0], inplace=True)\n",
    "\n",
    "    df.set_index(keys=df['Student'],inplace=True)\n",
    "    df.drop(index = ['Test Student'], inplace=True)\n",
    "\n",
    "    # Parse out Section Number\n",
    "    df['Section'] = df.Section.str.extract('(\\d+)')\n",
    "    df = df[df['Section']==sectionID]\n",
    "\n",
    "    df.drop(columns=['Student','ID','SIS User ID','SIS Login ID'],inplace=True)\n",
    "    df.drop(columns=['Assignments Current Points','Assignments Final Points','Assignments Unposted Current Score','Assignments Final Score'],inplace=True)\n",
    "    df.drop(columns=['Assignments Unposted Final Score','Imported Assignments Current Points','Imported Assignments Final Points','Imported Assignments Current Score'],inplace=True)\n",
    "    df.drop(columns=['Imported Assignments Unposted Current Score','Unposted Final Score','Final Score','Unposted Current Score'],inplace=True)\n",
    "    df.drop(columns=['Assignments Current Score','Imported Assignments Final Score','Imported Assignments Unposted Final Score','Current Points','Final Points'],inplace=True)\n",
    "\n",
    "    df = df.loc[:, df.isnull().mean() < .4]\n",
    "    df.fillna(0,inplace=True)\n",
    "    class_df = df.copy()\n",
    "    class_df['Current Score'] = df['Current Score'].apply(lambda x: float(x))\n",
    "\n",
    "    return class_df\n",
    "\n",
    "# Add Cluster Column to DataFrame\n",
    "def add_clusters(df, num_clusters=6):\n",
    "    '''\n",
    "    Add Clusters\n",
    "    '''\n",
    "    kmeans = KMeans(num_clusters)\n",
    "    kmeans.fit(df)\n",
    "    cluster = kmeans.predict(df)\n",
    "    df['Cluster'] = cluster\n",
    "    return df\n",
    "\n",
    "# Return a list of Clusters\n",
    "def return_cluster_list(df,num_clusters=6):\n",
    "    '''\n",
    "    Return a List of Clustered Students\n",
    "    '''\n",
    "    cluster_list = []\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        cluster_list.append(list(df[df['Cluster']==i].index))\n",
    "\n",
    "    return cluster_list\n",
    "\n",
    "\n",
    "# Make Student Strength/Growth Areas DataFrame\n",
    "def make_student_growth_and_strength_df(df_original,sectionID,cluster_labels):\n",
    "    '''\n",
    "    Create a DataFrame that includes students strengths and growth areas for question clusters\n",
    "\n",
    "    Return a List of Clustered Students\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    sectionID : Section Number for Students\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame with strengths and growth areas for question clusters\n",
    "\n",
    "    '''\n",
    "    df = df_original.copy()\n",
    "    clean_df = clean_file(df,sectionID)\n",
    "    quest_num_df = clean_df.iloc[:,0:-3]\n",
    "    quest_num_df.columns = cluster_labels\n",
    "    quest_num_df_grouped = quest_num_df.groupby(quest_num_df.columns, axis=1).sum()\n",
    "    grouped_quest_normed_df = normalize_df(quest_num_df_grouped)\n",
    "\n",
    "    x = grouped_quest_normed_df.copy().ix[0]\n",
    "    x.index[x.argmin()]\n",
    "    min_list, max_list = [], []\n",
    "\n",
    "    for i in range (len(grouped_quest_normed_df)):\n",
    "        x = grouped_quest_normed_df.ix[i]\n",
    "\n",
    "        min_list.append(x.index[x.argmin()])\n",
    "        max_list.append(x.index[x.argmax()])\n",
    "\n",
    "\n",
    "    grouped_quest_normed_df['Strength Area'] = max_list\n",
    "    grouped_quest_normed_df['Growth Area'] = min_list\n",
    "\n",
    "    return grouped_quest_normed_df\n",
    "\n",
    "\n",
    "# Generate Growth Areas Groups\n",
    "def generate_growth_groups(df,num_groups):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    student_df : DataFrame with student names as index and Strength/Growth Areas included\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Growth Area Groups\n",
    "    '''\n",
    "    index_list = list(df.index)\n",
    "\n",
    "    cluster_focus = []\n",
    "\n",
    "    for i in range(num_groups):\n",
    "\n",
    "        cluster_focus.append(list(df[df['Growth Area']==i].index))\n",
    "\n",
    "    print(\"Grouping by Growth Areas:\\n\")\n",
    "\n",
    "    for i,g in enumerate(cluster_focus):\n",
    "        print(\"Group\",i+1)\n",
    "        print(str(g)+\"\\n\")\n",
    "\n",
    "    return cluster_focus\n",
    "\n",
    "\n",
    "# Generate Strength Areas Groups\n",
    "def generate_strength_groups(df,num_groups):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    student_df : DataFrame with student names as index and Strength/Growth Areas included\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Growth Area Groups\n",
    "    '''\n",
    "    index_list = list(df.index)\n",
    "\n",
    "    cluster_focus = []\n",
    "\n",
    "    for i in range(num_groups):\n",
    "\n",
    "        cluster_focus.append(list(df[df['Strength Area']==i].index))\n",
    "\n",
    "    print(\"Grouping by Strength Areas:\\n\")\n",
    "\n",
    "    for i,g in enumerate(cluster_focus):\n",
    "        print(\"Group\",i+1)\n",
    "        print(str(g)+\"\\n\")\n",
    "\n",
    "    return cluster_focus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_assessment_df = pd.read_csv('../data/Student_Assessments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n",
      "ASSESSMENT 0 \n",
      "ASSESSMENT 1\n",
      "ASSESSMENT 2\n",
      "ASSESSMENT 3\n",
      "ASSESSMENT 4\n",
      "ASSESSMENT 5\n",
      "ASSESSMENT 6\n"
     ]
    }
   ],
   "source": [
    "student_df = clean_file(student_assessment_df)\n",
    "student_df = normalize_df(student_df)\n",
    "student_df = add_clusters(student_df, num_clusters = 4)\n",
    "\n",
    "result = return_cluster_list(student_df, num_clusters = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Black Widow',\n",
       "  'Swamp Thing',\n",
       "  'Iron Man',\n",
       "  'Rocket Raccoon',\n",
       "  'Steel',\n",
       "  'Catwoman'],\n",
       " ['Storm',\n",
       "  'Deadpool',\n",
       "  'Wolverine',\n",
       "  'Thing',\n",
       "  'Doctor Doom',\n",
       "  'Elektra',\n",
       "  'Green Arrow'],\n",
       " ['Ant-Man',\n",
       "  'Mystique',\n",
       "  'Groot',\n",
       "  'Superman',\n",
       "  'Superwoman',\n",
       "  'Mister Fantastic',\n",
       "  'Nightcrawler',\n",
       "  'Cyclops'],\n",
       " ['Batman', 'Susan Storm', 'Wonder Woman']]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0\n",
      "Average 33.27\n",
      "Students ['Black Widow', 'Swamp Thing', 'Iron Man', 'Rocket Raccoon', 'Steel', 'Catwoman']\n",
      "Cluster  1\n",
      "Average 84.17\n",
      "Students ['Storm', 'Deadpool', 'Wolverine', 'Thing', 'Doctor Doom', 'Elektra', 'Green Arrow']\n",
      "Cluster  2\n",
      "Average 71.7\n",
      "Students ['Ant-Man', 'Mystique', 'Groot', 'Superman', 'Superwoman', 'Mister Fantastic', 'Nightcrawler', 'Cyclops']\n",
      "Cluster  3\n",
      "Average 52.02\n",
      "Students ['Batman', 'Susan Storm', 'Wonder Woman']\n"
     ]
    }
   ],
   "source": [
    "student_dict = dict()\n",
    "cluster_averages = dict()\n",
    "\n",
    "for i,val in enumerate(result):\n",
    "    print (\"Cluster \",i)\n",
    "    avg = round(np.mean(np.mean(student_df.loc[val]).values)*100, 2)\n",
    "#     avg = ((round(np.mean(student_df.loc[val]['ASSESSMENT 0'])*100,2)) +\"%<br/>\")\n",
    "    print (\"Average\", avg)\n",
    "    print (\"Students\", val)\n",
    "    student_dict[i] = val\n",
    "    cluster_averages[i] = avg\n",
    "    \n",
    "    \n",
    "#     groups_string += \"<br/>Cluster \" + str(i+1) + \":<br/>\"\n",
    "#     groups_string += str(list(val)) + \"<br/>\"\n",
    "#     groups_string += (\"Average Score: \" +\n",
    "#         str(round(np.mean(student_df.loc[val]['score'])*100,2)) +\"%<br/>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 33.27, 1: 84.17, 2: 71.7, 3: 52.02}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(assessment_df, num_clusters = 3):\n",
    "    results_array = []\n",
    "    student_dict = dict()\n",
    "    cluster_averages = dict()\n",
    "    \n",
    "    student_df = clean_file(assessment_df)\n",
    "    student_df = normalize_df(student_df)\n",
    "    student_df = add_clusters(student_df, num_clusters = num_clusters)\n",
    "    results_array.append(student_df)\n",
    "    \n",
    "    result = return_cluster_list(student_df, num_clusters = num_clusters)\n",
    "    results_array.append(result)\n",
    "    \n",
    "    for i,val in enumerate(result):\n",
    "        avg = round(np.mean(np.mean(student_df.loc[val]).values)*100, 2)\n",
    "        student_dict[i] = val\n",
    "        cluster_averages[i] = avg\n",
    "    \n",
    "    results_array.append(student_dict)\n",
    "    results_array.append(cluster_averages)\n",
    "\n",
    "    return results_array    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n",
      "ASSESSMENT 0 \n",
      "ASSESSMENT 1\n",
      "ASSESSMENT 2\n",
      "ASSESSMENT 3\n",
      "ASSESSMENT 4\n",
      "ASSESSMENT 5\n",
      "ASSESSMENT 6\n"
     ]
    }
   ],
   "source": [
    "student_assessment_df = pd.read_csv('../data/Student_Assessments.csv')\n",
    "temp_arr = create_clusters(student_assessment_df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 47.96, 1: 27.02, 2: 96.67, 3: 71.43}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_arr[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_file = pd.read_csv('../data/students.csv')\n",
    "past_pairs = pd.read_csv('../data/past_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(students_file,past_pairs_file):\n",
    "    print(students_file)\n",
    "    print(past_pairs_file)\n",
    "    ! python pairs.py students_file, past_pairs_file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Students Average\n",
      "0    Chinmay  57.00%\n",
      "1      Chris  77.25%\n",
      "2     Daniel  53.50%\n",
      "3      David  56.00%\n",
      "4     Farzad  59.25%\n",
      "5       Hank  86.00%\n",
      "6     Haydin  60.00%\n",
      "7      Jared  85.50%\n",
      "8     Justin  52.25%\n",
      "9      Kevin  72.25%\n",
      "10  Lawrence  72.00%\n",
      "11      Mark  38.50%\n",
      "12      Neel  62.25%\n",
      "13    Nicole  65.75%\n",
      "14     Purvi  58.75%\n",
      "15     Rahat  57.50%\n",
      "16     Agnes  74.75%\n",
      "17   Suchaya  67.50%\n",
      "18      Todd  46.25%\n",
      "19   Winrich  70.00%\n",
      "20    Brooks  84.00%\n",
      "21  Yue Weng  67.50%\n",
      "22      John  58.50%\n",
      "23       Zoe  40.00%\n",
      "        pair1    pair2 pair3   date\n",
      "0       Agnes   Brooks   NaN  2-Dec\n",
      "1        Neel  Winrich   NaN    NaN\n",
      "2       Chris   Nicole   NaN    NaN\n",
      "3     Chinmay   Justin   NaN    NaN\n",
      "4       Purvi      Ais   NaN    NaN\n",
      "..        ...      ...   ...    ...\n",
      "263     Kevin   Haydin   NaN    NaN\n",
      "264     Rahat  Suchaya   NaN    NaN\n",
      "265  Lawrence   Daniel   NaN    NaN\n",
      "266    Justin   Nicole   NaN    NaN\n",
      "267     Purvi   Brooks   NaN    NaN\n",
      "\n",
      "[268 rows x 4 columns]\n",
      "\u001b[H\u001b[2J\n",
      "Usage: python pairs.py [optional argument: number of random combinations]\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pairs.py\", line 369, in <module>\n",
      "    pairs = Pairs(samples, students_file, past_pairs_file, new_pairs_file, verbose)\n",
      "  File \"pairs.py\", line 27, in __init__\n",
      "    self._load_students(students_file)\n",
      "  File \"pairs.py\", line 36, in _load_students\n",
      "    self.students = (pd.read_csv(students_file)\n",
      "  File \"/Users/jackvessa/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\", line 685, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/jackvessa/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\", line 457, in _read\n",
      "    parser = TextFileReader(fp_or_buf, **kwds)\n",
      "  File \"/Users/jackvessa/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\", line 895, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"/Users/jackvessa/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"/Users/jackvessa/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1917, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n",
      "  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\n",
      "FileNotFoundError: [Errno 2] File b'students_file,' does not exist: b'students_file,'\n"
     ]
    }
   ],
   "source": [
    "generate_pairs(students_file,past_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2Jstudents_file,\r\n",
      "\r\n",
      "Usage: python pairs.py [optional argument: number of random combinations]\r\n",
      "\r\n",
      "      PAIR GENERATOR\r\n",
      "\r\n",
      "Number of random combinations: past_pairs \r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"pairs.py\", line 372, in <module>\r\n",
      "    newPairs, newTrios = pairs.find_min()\r\n",
      "  File \"pairs.py\", line 62, in find_min\r\n",
      "    for _ in range(self.samples):\r\n",
      "TypeError: 'str' object cannot be interpreted as an integer\r\n"
     ]
    }
   ],
   "source": [
    "!python pairs.py students_file, past_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(students_file,past_pairs_file):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
