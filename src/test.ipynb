{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "# Imports for NLP Analysis of Columns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create Group Sizes\n",
    "def calc_group_sizes(num_students, num_groups):\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    num_students : int\n",
    "        Number of students in the class\n",
    "    num_groups : int\n",
    "        Number of groups to break students into\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    group_size : List of ideal group sizes\n",
    "    '''\n",
    "    group_sizes = []\n",
    "\n",
    "    class_size = int(num_students)\n",
    "    group_num_count = int(num_groups)\n",
    "    group_num = int(num_groups)\n",
    "\n",
    "    for i in range(group_num_count):\n",
    "        temp = class_size // group_num\n",
    "        class_size -= temp\n",
    "        group_num -= 1\n",
    "        group_sizes.append(temp)\n",
    "\n",
    "    return group_sizes\n",
    "\n",
    "# Clean DataFrame by Section\n",
    "def clean_file(dataframe):\n",
    "    '''\n",
    "    Clean CSV file\n",
    "    --------------------\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    .csv file :\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    Pandas DataFrame (Cleaned)\n",
    "    '''\n",
    "    df = dataframe.copy()\n",
    "    df.set_index(keys=df['FIRST'],inplace=True)\n",
    "\n",
    "    try:\n",
    "        df.drop(columns=['FIRST','LAST','GITHUB','Class Rank','Overall Average'],inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].str.rstrip('%').astype('float') / 100.0\n",
    "    \n",
    "#     df = df.select_dtypes(exclude=['object','bool'])\n",
    "\n",
    "#     df.drop(columns=['id','section_sis_id','attempt'],inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# def clean_file_galvanize(df):\n",
    "#     '''\n",
    "#     '''\n",
    "#     temp_df = df.copy()\n",
    "#     temp_df.drop(columns=['LAST', 'GITHUB','Class Rank'], inplace=True)\n",
    "\n",
    "#     temp_df.set_index('FIRST', inplace=True)\n",
    "\n",
    "#     for col in temp_df.columns:\n",
    "\n",
    "#         temp_df[col] = temp_df[col].apply(lambda x: float((str(x).replace(\"%\",\"\"))))\n",
    "\n",
    "#     return temp_df\n",
    "\n",
    "# Normalize DataFrame (0-1)\n",
    "def normalize_df(df):\n",
    "    '''\n",
    "    Normalize DataFrame Values from 0-1\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame to Normalize\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Normalized DataFrame\n",
    "    '''\n",
    "    return df.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)) \\\n",
    "        if np.min(x) != np.max(x) else x)\n",
    "\n",
    "# Add Cluster Column to DataFrame\n",
    "def add_clusters(df, num_clusters=6):\n",
    "    '''\n",
    "    Add Clusters\n",
    "    '''\n",
    "    kmeans = KMeans(num_clusters)\n",
    "    kmeans.fit(df)\n",
    "    cluster = kmeans.predict(df)\n",
    "    df['Cluster'] = cluster\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_clusters(assessment_df, num_clusters = 3):\n",
    "    results_array = []\n",
    "    student_dict = dict()\n",
    "    cluster_averages = dict()\n",
    "    \n",
    "    student_df = clean_file(assessment_df)\n",
    "    student_df = normalize_df(student_df)\n",
    "    student_df = add_clusters(student_df, num_clusters = num_clusters)\n",
    "    results_array.append(student_df)\n",
    "    \n",
    "    result = return_cluster_list(student_df, num_clusters = num_clusters)\n",
    "    results_array.append(result)\n",
    "    \n",
    "    for i,val in enumerate(result):\n",
    "        avg = round(np.mean(np.mean(student_df.loc[val].iloc[:,:-1]).values)*100, 2)\n",
    "        student_dict[i] = val\n",
    "        cluster_averages[i] = avg\n",
    "    \n",
    "    results_array.append(student_dict)\n",
    "    results_array.append(cluster_averages)\n",
    "\n",
    "    clusters, scores = generate_clusters_and_scores(results_array)\n",
    "    \n",
    "    return clusters, scores    \n",
    "\n",
    "\n",
    "def generate_clusters_and_scores(temp_arr):\n",
    "    \n",
    "    cluster_students_array = []\n",
    "    cluster_students_scores_array = []\n",
    "\n",
    "    for key, cluster in temp_arr[2].items():\n",
    "        cluster_students_array.append(cluster)\n",
    "        \n",
    "    for key, score in temp_arr[3].items():\n",
    "        cluster_students_scores_array.append(score)\n",
    "        \n",
    "    return cluster_students_array, cluster_students_scores_array\n",
    "\n",
    "\n",
    "# Generate Optimized Group (Based on Residual Sum of Squares)\n",
    "def generate_optimized_groups(student_df, num_iter = 100, num_groups = 6, Homogeneous = 0, criteria = 'score'):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    student_df : DataFrame with student names as index and score column\n",
    "    num_iter : int\n",
    "        Number of Iterations to run loss function\n",
    "    num_groups : int\n",
    "        Number of groups to divide students into\n",
    "    Homogeneous : bool\n",
    "        If True, create Homogeneous (similar) groups.\n",
    "        If False, create Heterogeneous (different) groups\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Optimal Groups\n",
    "    '''\n",
    "    index_list = list(student_df.index)\n",
    "\n",
    "    if Homogeneous == 0:\n",
    "        ideal_loss = 9999\n",
    "    elif Homogeneous == 1:\n",
    "        ideal_loss = 0\n",
    "    num_students = len(student_df)\n",
    "\n",
    "    size_list = calc_group_sizes(num_students,num_groups)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        randomized_index_list = np.random.choice(index_list, size = len(index_list),replace=False)\n",
    "        group_set = set({})\n",
    "        index_track = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for num in size_list:\n",
    "            j = frozenset(randomized_index_list[0 + index_track:index_track+num])\n",
    "            group_set.add(j)\n",
    "            index_track += num\n",
    "\n",
    "        for group in group_set:\n",
    "            unfrozen = set(group)\n",
    "            group_loss = 0\n",
    "            avg_score = np.mean(student_df.loc[unfrozen][criteria])\n",
    "\n",
    "            for s in range(len(group)):\n",
    "                group_loss += (student_df.loc[unfrozen][criteria][s] - avg_score) ** 2\n",
    "\n",
    "            total_loss += group_loss\n",
    "\n",
    "        if Homogeneous == 0 and total_loss < ideal_loss:\n",
    "            ideal_loss = total_loss\n",
    "            best_group = group_set\n",
    "            print(\"New Best Homogeneous Group Loss:\", ideal_loss)\n",
    "\n",
    "        elif Homogeneous == 1 and total_loss > ideal_loss:\n",
    "            ideal_loss = total_loss\n",
    "            best_group = group_set\n",
    "            print(\"New Best Heterogeneous Group Loss:\", ideal_loss)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Final Best Group Loss:\", ideal_loss)\n",
    "    print(\"Final Best Grouping:\\n\")\n",
    "\n",
    "\n",
    "    for i,g in enumerate(best_group):\n",
    "        print(\"Group\",i+1)\n",
    "        print(student_df.loc[set(g)][criteria],\"\\n\")\n",
    "\n",
    "    return best_group\n",
    "\n",
    "\n",
    "# Make Student Strength/Growth Areas DataFrame\n",
    "def make_student_growth_and_strength_df(df_original,sectionID,cluster_labels):\n",
    "    '''\n",
    "    Create a DataFrame that includes students strengths and growth areas for question clusters\n",
    "\n",
    "    Return a List of Clustered Students\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : Pandas DataFrame\n",
    "    sectionID : Section Number for Students\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas DataFrame with strengths and growth areas for question clusters\n",
    "\n",
    "    '''\n",
    "    df = df_original.copy()\n",
    "    clean_df = clean_file(df,sectionID)\n",
    "    quest_num_df = clean_df.iloc[:,0:-3]\n",
    "    quest_num_df.columns = cluster_labels\n",
    "    quest_num_df_grouped = quest_num_df.groupby(quest_num_df.columns, axis=1).sum()\n",
    "    grouped_quest_normed_df = normalize_df(quest_num_df_grouped)\n",
    "\n",
    "    x = grouped_quest_normed_df.copy().ix[0]\n",
    "    x.index[x.argmin()]\n",
    "    min_list, max_list = [], []\n",
    "\n",
    "    for i in range (len(grouped_quest_normed_df)):\n",
    "        x = grouped_quest_normed_df.ix[i]\n",
    "\n",
    "        min_list.append(x.index[x.argmin()])\n",
    "        max_list.append(x.index[x.argmax()])\n",
    "\n",
    "\n",
    "    grouped_quest_normed_df['Strength Area'] = max_list\n",
    "    grouped_quest_normed_df['Growth Area'] = min_list\n",
    "\n",
    "    return grouped_quest_normed_df\n",
    "\n",
    "\n",
    "# Generate Growth Areas Groups\n",
    "def generate_growth_groups(df,num_groups):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    student_df : DataFrame with student names as index and Strength/Growth Areas included\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Growth Area Groups\n",
    "    '''\n",
    "    index_list = list(df.index)\n",
    "\n",
    "    cluster_focus = []\n",
    "\n",
    "    for i in range(num_groups):\n",
    "\n",
    "        cluster_focus.append(list(df[df['Growth Area']==i].index))\n",
    "\n",
    "    print(\"Grouping by Growth Areas:\\n\")\n",
    "\n",
    "    for i,g in enumerate(cluster_focus):\n",
    "        print(\"Group\",i+1)\n",
    "        print(str(g)+\"\\n\")\n",
    "\n",
    "    return cluster_focus\n",
    "\n",
    "\n",
    "# Generate Strength Areas Groups\n",
    "def generate_strength_groups(df,num_groups):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    student_df : DataFrame with student names as index and Strength/Growth Areas included\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Growth Area Groups\n",
    "    '''\n",
    "    index_list = list(df.index)\n",
    "\n",
    "    cluster_focus = []\n",
    "\n",
    "    for i in range(num_groups):\n",
    "\n",
    "        cluster_focus.append(list(df[df['Strength Area']==i].index))\n",
    "\n",
    "    print(\"Grouping by Strength Areas:\\n\")\n",
    "\n",
    "    for i,g in enumerate(cluster_focus):\n",
    "        print(\"Group\",i+1)\n",
    "        print(str(g)+\"\\n\")\n",
    "\n",
    "    return cluster_focus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_assessment_df = pd.read_csv('../data/Student_Assessments.csv')\n",
    "clusters, scores = create_clusters(student_assessment_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ant-Man',\n",
       "  'Iron Man',\n",
       "  'Mystique',\n",
       "  'Groot',\n",
       "  'Superman',\n",
       "  'Superwoman',\n",
       "  'Mister Fantastic',\n",
       "  'Nightcrawler',\n",
       "  'Cyclops'],\n",
       " ['Storm',\n",
       "  'Deadpool',\n",
       "  'Wolverine',\n",
       "  'Thing',\n",
       "  'Doctor Doom',\n",
       "  'Elektra',\n",
       "  'Green Arrow'],\n",
       " ['Black Widow',\n",
       "  'Swamp Thing',\n",
       "  'Rocket Raccoon',\n",
       "  'Steel',\n",
       "  'Batman',\n",
       "  'Susan Storm',\n",
       "  'Catwoman',\n",
       "  'Wonder Woman']]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52.41, 81.91, 29.16]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "students_file = pd.read_csv('../data/students.csv')\n",
    "past_pairs = pd.read_csv('../data/past_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(students_file,past_pairs_file):\n",
    "    print(students_file)\n",
    "    print(past_pairs_file)\n",
    "    ! python pairs.py students_file, past_pairs_file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Students Average\n",
      "0    Chinmay  57.00%\n",
      "1      Chris  77.25%\n",
      "2     Daniel  53.50%\n",
      "3      David  56.00%\n",
      "4     Farzad  59.25%\n",
      "5       Hank  86.00%\n",
      "6     Haydin  60.00%\n",
      "7      Jared  85.50%\n",
      "8     Justin  52.25%\n",
      "9      Kevin  72.25%\n",
      "10  Lawrence  72.00%\n",
      "11      Mark  38.50%\n",
      "12      Neel  62.25%\n",
      "13    Nicole  65.75%\n",
      "14     Purvi  58.75%\n",
      "15     Rahat  57.50%\n",
      "16     Agnes  74.75%\n",
      "17   Suchaya  67.50%\n",
      "18      Todd  46.25%\n",
      "19   Winrich  70.00%\n",
      "20    Brooks  84.00%\n",
      "21  Yue Weng  67.50%\n",
      "22      John  58.50%\n",
      "23       Zoe  40.00%\n",
      "        pair1    pair2 pair3   date\n",
      "0       Agnes   Brooks   NaN  2-Dec\n",
      "1        Neel  Winrich   NaN    NaN\n",
      "2       Chris   Nicole   NaN    NaN\n",
      "3     Chinmay   Justin   NaN    NaN\n",
      "4       Purvi      Ais   NaN    NaN\n",
      "..        ...      ...   ...    ...\n",
      "263     Kevin   Haydin   NaN    NaN\n",
      "264     Rahat  Suchaya   NaN    NaN\n",
      "265  Lawrence   Daniel   NaN    NaN\n",
      "266    Justin   Nicole   NaN    NaN\n",
      "267     Purvi   Brooks   NaN    NaN\n",
      "\n",
      "[268 rows x 4 columns]\n",
      "\u001b[H\u001b[2J\n",
      "Usage: python pairs.py [optional argument: number of random combinations]\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pairs.py\", line 369, in <module>\n",
      "    pairs = Pairs(samples, students_file, past_pairs_file, new_pairs_file, verbose)\n",
      "  File \"pairs.py\", line 27, in __init__\n",
      "    self._load_students(students_file)\n",
      "  File \"pairs.py\", line 36, in _load_students\n",
      "    self.students = (pd.read_csv(students_file)\n",
      "  File \"/Users/jackvessa/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\", line 685, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"/Users/jackvessa/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\", line 457, in _read\n",
      "    parser = TextFileReader(fp_or_buf, **kwds)\n",
      "  File \"/Users/jackvessa/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\", line 895, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"/Users/jackvessa/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1135, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"/Users/jackvessa/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1917, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"pandas/_libs/parsers.pyx\", line 382, in pandas._libs.parsers.TextReader.__cinit__\n",
      "  File \"pandas/_libs/parsers.pyx\", line 689, in pandas._libs.parsers.TextReader._setup_parser_source\n",
      "FileNotFoundError: [Errno 2] File b'students_file,' does not exist: b'students_file,'\n"
     ]
    }
   ],
   "source": [
    "generate_pairs(students_file,past_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2Jstudents_file,\r\n",
      "\r\n",
      "Usage: python pairs.py [optional argument: number of random combinations]\r\n",
      "\r\n",
      "      PAIR GENERATOR\r\n",
      "\r\n",
      "Number of random combinations: past_pairs \r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"pairs.py\", line 372, in <module>\r\n",
      "    newPairs, newTrios = pairs.find_min()\r\n",
      "  File \"pairs.py\", line 62, in find_min\r\n",
      "    for _ in range(self.samples):\r\n",
      "TypeError: 'str' object cannot be interpreted as an integer\r\n"
     ]
    }
   ],
   "source": [
    "!python pairs.py students_file, past_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(students_file,past_pairs_file):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfigParser\n",
      "Cython\n",
      "IPython\n",
      "OpenSSL\n",
      "PIL\n",
      "Parser\n",
      "PyQt5\n",
      "Queue\n",
      "SocketServer\n",
      "__future__\n",
      "__main__\n",
      "_abc\n",
      "_ast\n",
      "_asyncio\n",
      "_bisect\n",
      "_blake2\n",
      "_bootlocale\n",
      "_bz2\n",
      "_cffi_backend\n",
      "_codecs\n",
      "_collections\n",
      "_collections_abc\n",
      "_compat_pickle\n",
      "_compression\n",
      "_contextvars\n",
      "_csv\n",
      "_ctypes\n",
      "_curses\n",
      "_datetime\n",
      "_decimal\n",
      "_dummy_thread\n",
      "_elementtree\n",
      "_functools\n",
      "_hashlib\n",
      "_heapq\n",
      "_imp\n",
      "_io\n",
      "_json\n",
      "_locale\n",
      "_lsprof\n",
      "_lzma\n",
      "_markupbase\n",
      "_md5\n",
      "_multiprocessing\n",
      "_opcode\n",
      "_operator\n",
      "_osx_support\n",
      "_pickle\n",
      "_posixsubprocess\n",
      "_py_abc\n",
      "_pydecimal\n",
      "_pytest\n",
      "_queue\n",
      "_random\n",
      "_scproxy\n",
      "_sha1\n",
      "_sha256\n",
      "_sha3\n",
      "_sha512\n",
      "_signal\n",
      "_sitebuiltins\n",
      "_socket\n",
      "_sqlite3\n",
      "_sre\n",
      "_ssl\n",
      "_stat\n",
      "_string\n",
      "_strptime\n",
      "_struct\n",
      "_thread\n",
      "_threading_local\n",
      "_tkinter\n",
      "_tracemalloc\n",
      "_uuid\n",
      "_warnings\n",
      "_weakref\n",
      "_weakrefset\n",
      "abc\n",
      "appnope\n",
      "argparse\n",
      "array\n",
      "asn1crypto\n",
      "ast\n",
      "asyncio\n",
      "atexit\n",
      "atomicwrites\n",
      "attr\n",
      "audioop\n",
      "babel\n",
      "backcall\n",
      "backports\n",
      "base64\n",
      "bdb\n",
      "binascii\n",
      "bisect\n",
      "botocore\n",
      "bs4\n",
      "builtins\n",
      "bz2\n",
      "cProfile\n",
      "calendar\n",
      "certifi\n",
      "cffi\n",
      "cgi\n",
      "chardet\n",
      "chunk\n",
      "click\n",
      "cloudpickle\n",
      "cmd\n",
      "code\n",
      "codecs\n",
      "codeop\n",
      "collections\n",
      "colorama\n",
      "colorsys\n",
      "concurrent\n",
      "configparser\n",
      "contextlib\n",
      "contextlib2\n",
      "contextvars\n",
      "copy\n",
      "copyreg\n",
      "cryptography\n",
      "csv\n",
      "ctypes\n",
      "curses\n",
      "cycler\n",
      "cython\n",
      "dataclasses\n",
      "datetime\n",
      "dateutil\n",
      "decimal\n",
      "decorator\n",
      "defusedxml\n",
      "difflib\n",
      "dis\n",
      "distutils\n",
      "doctest\n",
      "docutils\n",
      "dummy_threading\n",
      "email\n",
      "encodings\n",
      "entrypoints\n",
      "enum\n",
      "errno\n",
      "et_xmlfile\n",
      "faulthandler\n",
      "fcntl\n",
      "filecmp\n",
      "fileinput\n",
      "flask\n",
      "fnmatch\n",
      "fractions\n",
      "ftplib\n",
      "functools\n",
      "gc\n",
      "genericpath\n",
      "getopt\n",
      "getpass\n",
      "gettext\n",
      "glob\n",
      "greenlet\n",
      "grp\n",
      "gzip\n",
      "hashlib\n",
      "heapq\n",
      "hmac\n",
      "html\n",
      "html5lib\n",
      "http\n",
      "idna\n",
      "imp\n",
      "importlib\n",
      "importlib_metadata\n",
      "inspect\n",
      "io\n",
      "ipaddress\n",
      "ipykernel\n",
      "ipython_genutils\n",
      "itertools\n",
      "itsdangerous\n",
      "jdcal\n",
      "jedi\n",
      "jinja2\n",
      "json\n",
      "jsonschema\n",
      "jupyter_client\n",
      "jupyter_core\n",
      "keyword\n",
      "kiwisolver\n",
      "lib2to3\n",
      "linecache\n",
      "locale\n",
      "logging\n",
      "lxml\n",
      "lzma\n",
      "markupsafe\n",
      "marshal\n",
      "math\n",
      "matplotlib\n",
      "mimetypes\n",
      "mistune\n",
      "mkl\n",
      "mkl_fft\n",
      "mmap\n",
      "more_itertools\n",
      "multiprocessing\n",
      "nbconvert\n",
      "nbformat\n",
      "netrc\n",
      "nose\n",
      "notebook\n",
      "ntpath\n",
      "nturl2path\n",
      "numbers\n",
      "numexpr\n",
      "numpy\n",
      "numpydoc\n",
      "opcode\n",
      "openpyxl\n",
      "operator\n",
      "optparse\n",
      "os\n",
      "packaging\n",
      "pandas\n",
      "pandocfilters\n",
      "parser\n",
      "parso\n",
      "pathlib\n",
      "pathlib2\n",
      "pdb\n",
      "pexpect\n",
      "pickle\n",
      "pickleshare\n",
      "pipes\n",
      "pkg_resources\n",
      "pkgutil\n",
      "platform\n",
      "plistlib\n",
      "pluggy\n",
      "posix\n",
      "posixpath\n",
      "pprint\n",
      "profile\n",
      "prompt_toolkit\n",
      "pstats\n",
      "psutil\n",
      "pty\n",
      "ptyprocess\n",
      "pvectorc\n",
      "pwd\n",
      "py\n",
      "py_compile\n",
      "pycparser\n",
      "pydoc\n",
      "pydoc_data\n",
      "pyexpat\n",
      "pygments\n",
      "pyparsing\n",
      "pyrsistent\n",
      "pytest\n",
      "pytz\n",
      "pyximport\n",
      "qtpy\n",
      "queue\n",
      "quopri\n",
      "random\n",
      "re\n",
      "readline\n",
      "reprlib\n",
      "requests\n",
      "resource\n",
      "rlcompleter\n",
      "runpy\n",
      "scipy\n",
      "select\n",
      "selectors\n",
      "setuptools\n",
      "shlex\n",
      "shutil\n",
      "signal\n",
      "sip\n",
      "site\n",
      "six\n",
      "smtplib\n",
      "socket\n",
      "socketserver\n",
      "socks\n",
      "soupsieve\n",
      "sphinx\n",
      "sqlalchemy\n",
      "sqlite3\n",
      "sre_compile\n",
      "sre_constants\n",
      "sre_parse\n",
      "ssl\n",
      "stat\n",
      "string\n",
      "stringprep\n",
      "struct\n",
      "subprocess\n",
      "sys\n",
      "sysconfig\n",
      "tables\n",
      "tarfile\n",
      "tempfile\n",
      "termcolor\n",
      "termios\n",
      "testpath\n",
      "tests\n",
      "textwrap\n",
      "threading\n",
      "time\n",
      "timeit\n",
      "tkinter\n",
      "token\n",
      "tokenize\n",
      "tornado\n",
      "traceback\n",
      "tracemalloc\n",
      "traitlets\n",
      "tty\n",
      "types\n",
      "typing\n",
      "unicodedata\n",
      "unittest\n",
      "urllib\n",
      "urllib3\n",
      "uu\n",
      "uuid\n",
      "warnings\n",
      "wave\n",
      "wcwidth\n",
      "weakref\n",
      "webbrowser\n",
      "webencodings\n",
      "werkzeug\n",
      "xlrd\n",
      "xlsxwriter\n",
      "xlwt\n",
      "xml\n",
      "xmlrpc\n",
      "zipfile\n",
      "zipimport\n",
      "zipp\n",
      "zlib\n",
      "zmq\n"
     ]
    }
   ],
   "source": [
    "from modulefinder import ModuleFinder\n",
    "f = ModuleFinder()\n",
    "# Run the main script\n",
    "f.run_script('../main.py')\n",
    "# Get names of all the imported modules\n",
    "names = list(f.modules.keys())\n",
    "# Get a sorted list of the root modules imported\n",
    "basemods = sorted(set([name.split('.')[0] for name in names]))\n",
    "# Print it nicely\n",
    "print (\"\\n\".join(basemods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
